import os
import google.generativeai as genai
from dotenv import load_dotenv
import PIL.Image
import re

# Carrega as vari√°veis de ambiente
load_dotenv()

try:
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        print("‚ùå Chave de API do Google (GOOGLE_API_KEY) n√£o encontrada.")
        genai.configure(api_key="CHAVE_INVALIDA")
    else:
        genai.configure(api_key=api_key)
except Exception as e:
    print(f"ü§Ø Erro ao configurar a API do Gemini: {e}")


def _definir_personalidade(humor: int, contexto_memoria: str = "") -> str:
    """Define a instru√ß√£o de sistema com base no humor e no contexto da mem√≥ria."""
    instrucao_base = "Voc√™ √© a assistente LIA."
    if humor <= 25:
        personalidade = "Responda de forma direta, clara, concisa e mais s√©ria, como uma assistente profissional."
    elif humor <= 75:
        personalidade = "Responda de forma amig√°vel e prestativa, mantendo um tom equilibrado."
    else:  # humor > 75
        personalidade = "Responda de forma bem-humorada, criativa e um pouco engra√ßada, usando toques de ironia ou piadas quando apropriado, mas sem deixar de ser √∫til."

    instrucao_final = f"{instrucao_base} {personalidade}"

    if contexto_memoria:
        instrucao_final += f"\n\n--- CONTEXTO IMPORTANTE DE MEM√ìRIAS RECENTES E FATOS SOBRE O USU√ÅRIO ---\n{contexto_memoria}\n--- FIM DO CONTEXTO ---"

    return instrucao_final


async def perguntar_ao_gpt(mensagem_usuario, humor_lia: int, contexto_memoria: str = ""):
    """
    Envia um prompt de TEXTO para o modelo Gemini, agora com contexto de mem√≥ria.
    """
    print(
        f"üß† Enviando prompt para o Google Gemini (Humor: {humor_lia}%, Contexto: {'Sim' if contexto_memoria else 'N√£o'})...")
    try:
        instrucao_sistema = _definir_personalidade(humor_lia, contexto_memoria)
        # --- USANDO O MODELO CORRETO E MODERNO ---
        model = genai.GenerativeModel(
            'models/gemini-2.5-flash',
            system_instruction=instrucao_sistema
        )
        response = await model.generate_content_async(mensagem_usuario)

        if not response.parts:
            print("‚ö†Ô∏è Resposta do Gemini foi bloqueada ou retornou vazia (provavelmente filtro de seguran√ßa).")
            return "N√£o posso responder a isso."

        print("‚úÖ Resposta de texto recebida do Gemini.")
        return response.text.strip()
    except Exception as e:
        print(f"ü§Ø Erro ao chamar a API do Gemini (texto): {e}")
        return "Desculpe, estou com problemas de conex√£o com minha IA."


async def summarize_memories_with_gpt(memories_text: str):
    """Envia um bloco de texto de mem√≥rias para ser resumido pela IA."""
    try:
        system_instruction = (
            "Voc√™ √© um especialista em an√°lise de logs de conversas. Sua tarefa √© ler a lista de eventos e conte√∫dos a seguir e "
            "resumi-la em 2 ou 3 pontos chave e concisos. Extraia apenas as informa√ß√µes mais importantes sobre os interesses, "
            "o contexto e os fatos principais sobre o usu√°rio. Responda apenas com os pontos chave, de forma impessoal."
        )
        # --- USANDO O MODELO CORRETO E MODERNO ---
        model = genai.GenerativeModel('models/gemini-2.5-flash', system_instruction=system_instruction)
        prompt = f"Analise e resuma os seguintes logs de intera√ß√£o com um usu√°rio:\n\n{memories_text}"
        response = await model.generate_content_async(prompt)

        if not response.parts:
            return ""
        return response.text.strip()
    except Exception as e:
        print(f"ü§Ø Erro ao chamar a API do Gemini para resumir mem√≥rias: {e}")
        return ""


async def extrair_fatos_da_memoria(memories_text: str):
    """Envia um bloco de mem√≥rias para a IA e pede para extrair fatos permanentes."""
    try:
        system_instruction = (
            "Voc√™ √© um analista de dados especialista em extrair informa√ß√µes nucleares de conversas. "
            "Sua tarefa √© ler os logs de intera√ß√£o a seguir e extrair APENAS fatos que parecem ser permanentes ou de longo prazo sobre o usu√°rio. "
            "Ignore informa√ß√µes tempor√°rias (como perguntas sobre o tempo de hoje ou cota√ß√µes de um dia espec√≠fico). "
            "Se um fato for aprendido, retorne-o em uma lista, onde cada fato est√° em uma nova linha e come√ßa com um h√≠fen. "
            "Exemplos de BONS fatos a extrair: '- O nome do usu√°rio √© Loops', '- O usu√°rio mora em Curitiba', '- O time de futebol do usu√°rio √© o Palmeiras'. "
            "Exemplos de informa√ß√µes RUINS para IGNORAR: '- O usu√°rio pediu uma piada', '- O usu√°rio perguntou as not√≠cias'. "
            "Se nenhum fato permanente for encontrado, retorne uma resposta vazia."
        )
        # --- USANDO O MODELO CORRETO E MODERNO ---
        model = genai.GenerativeModel('models/gemini-2.5-flash', system_instruction=system_instruction)
        prompt = f"Analise os seguintes logs e extraia os fatos permanentes:\n\n{memories_text}"
        response = await model.generate_content_async(prompt)

        if not response.parts:
            return []

        fatos = [line.strip().lstrip('-').strip() for line in response.text.strip().split('\n') if line.strip()]
        return fatos

    except Exception as e:
        print(f"ü§Ø Erro ao chamar a API do Gemini para extrair fatos: {e}")
        return []


async def descrever_imagem(caminho_imagem, prompt_texto):
    """
    Envia uma IMAGEM e um prompt de texto para o Gemini e trata respostas vazias.
    """
    print(f"üñºÔ∏è  Enviando imagem '{caminho_imagem}' para an√°lise do Gemini...")
    try:
        img = PIL.Image.open(caminho_imagem)
        # --- USANDO O MODELO CORRETO E MODERNO ---
        model = genai.GenerativeModel('models/gemini-2.5-flash')
        response = await model.generate_content_async([prompt_texto, img])

        if not response.parts:
            print("‚ö†Ô∏è An√°lise de imagem do Gemini foi bloqueada ou retornou vazia.")
            return "N√£o consegui processar o conte√∫do desta imagem."

        print("‚úÖ An√°lise de imagem recebida do Gemini.")
        return response.text.strip()
    except FileNotFoundError:
        print(f"‚ùå Erro: Imagem n√£o encontrada em '{caminho_imagem}'")
        return "Desculpe, n√£o consegui encontrar o arquivo de imagem."
    except Exception as e:
        print(f"ü§Ø Erro ao chamar a API do Gemini (imagem): {e}")
        return "Desculpe, n√£o consegui analisar a imagem."


async def gerar_codigo_com_gpt(prompt_usuario: str) -> str:
    """
    Envia um prompt para o Gemini com foco em gerar apenas c√≥digo Python.
    """
    print("ü§ñ Enviando prompt de gera√ß√£o de c√≥digo para o Google Gemini...")
    try:
        # --- USANDO O MODELO CORRETO E MODERNO ---
        model = genai.GenerativeModel(
            'models/gemini-2.5-flash',
            system_instruction=(
                "Voc√™ √© um assistente de programa√ß√£o especialista em Python. "
                "Sua tarefa √© gerar APENAS o c√≥digo Python funcional que resolve o pedido do usu√°rio. "
                "N√£o inclua explica√ß√µes, coment√°rios desnecess√°rios, ou a palavra 'python' no in√≠cio do c√≥digo. "
                "O c√≥digo deve ser completo e pronto para ser executado."
            )
        )
        response = await model.generate_content_async(f"Crie um script Python que fa√ßa o seguinte: {prompt_usuario}")

        if not response.parts:
            print("‚ö†Ô∏è Gera√ß√£o de c√≥digo bloqueada ou retornou vazia.")
            return None

        codigo_limpo = re.sub(r'^```python\s*|\s*```$', '', response.text.strip(), flags=re.MULTILINE)

        print("‚úÖ C√≥digo recebido do Gemini.")
        return codigo_limpo
    except Exception as e:
        print(f"ü§Ø Erro ao chamar a API do Gemini (c√≥digo): {e}")
        return None


async def alterar_codigo_com_gpt(codigo_anterior: str, pedido_de_alteracao: str) -> str:
    """
    Envia um c√≥digo existente e um pedido de altera√ß√£o para a IA.
    """
    print("ü§ñ Enviando prompt de altera√ß√£o de c√≥digo para o Google Gemini...")
    try:
        # --- USANDO O MODELO CORRETO E MODERNO ---
        model = genai.GenerativeModel(
            'models/gemini-2.5-flash',
            system_instruction=(
                "Voc√™ √© um assistente de programa√ß√£o especialista em Python. "
                "Sua tarefa √© modificar o c√≥digo Python fornecido de acordo com o pedido do usu√°rio. "
                "Retorne APENAS o c√≥digo Python completo e modificado. "
                "N√£o inclua explica√ß√µes, coment√°rios desnecess√°rios, ou a palavra 'python' no in√≠cio do c√≥digo."
            )
        )
        prompt_completo = (
            f"Aqui est√° um c√≥digo Python:\n\n```python\n{codigo_anterior}\n```\n\n"
            f"Por favor, modifique este c√≥digo para fazer o seguinte: {pedido_de_alteracao}"
        )
        response = await model.generate_content_async(prompt_completo)

        if not response.parts:
            print("‚ö†Ô∏è Altera√ß√£o de c√≥digo bloqueada ou retornou vazia.")
            return None

        codigo_limpo = re.sub(r'^```python\s*|\s*```$', '', response.text.strip(), flags=re.MULTILINE)

        print("‚úÖ C√≥digo alterado recebido do Gemini.")
        return codigo_limpo
    except Exception as e:
        print(f"ü§Ø Erro ao chamar a API do Gemini (altera√ß√£o de c√≥digo): {e}")
        return None